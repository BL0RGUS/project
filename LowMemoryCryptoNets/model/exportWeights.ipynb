{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_masks(img_width, kernel_size, out_channels=16):\n",
    "    masks = []\n",
    "    for k in range(kernel_size**2):\n",
    "        mask = []\n",
    "        rows = 0   \n",
    "        ## first row of mask, ll 0s if in first 3 kernels\n",
    "        if k < kernel_size:\n",
    "            for i in range(img_width):\n",
    "                mask.append(0)\n",
    "            rows += 1\n",
    "        \n",
    "        # middle rows, a and b are values of the left and rightmost columns\n",
    "        a = int(k % kernel_size != 0)\n",
    "        b = int((k+1) % kernel_size != 0)\n",
    "        for j in range(img_width-rows):\n",
    "            mask.append(a)\n",
    "            for i in range(img_width-2):\n",
    "                mask.append(1)\n",
    "            mask.append(b)\n",
    "            rows += 1\n",
    "        \n",
    "        # last row\n",
    "        if k >= kernel_size*(kernel_size-1):\n",
    "            for j in range(img_width):\n",
    "                mask[(img_width**2)-1-j] = 0\n",
    "                \n",
    "        masks.append(np.tile(np.array(mask), out_channels))\n",
    "    return masks\n",
    "'''\n",
    "def build_mask(starting_padding, ending_padding, window_length, max_length):\n",
    "    mask = []\n",
    "    for i in range(starting_padding):\n",
    "        mask.append(0)\n",
    "    while len(mask) < (max_length - ending_padding):\n",
    "        for j in range(window_length):\n",
    "            mask.append(1)\n",
    "        mask.append(0)\n",
    "        \n",
    "    while len(mask) > max_length:\n",
    "        mask.pop()\n",
    "    while len(mask) < max_length:\n",
    "        mask.append(0)\n",
    "        \n",
    "    for i in range(ending_padding):\n",
    "        mask[max_length - i - 1] = 0\n",
    "        \n",
    "    return mask\n",
    "\n",
    "\n",
    "def build_masks(img_width, kernel_size, channels_in):\n",
    "    masks = []\n",
    "    masks.append(np.tile(np.array(build_mask(img_width + 1, 0, img_width -1, img_width ** 2)), channels_in))\n",
    "    masks.append(np.tile(np.array(build_mask(img_width, 0, img_width ** 2, img_width ** 2)), channels_in))\n",
    "    masks.append(np.tile(np.array(build_mask(img_width, 0, img_width - 1, img_width ** 2)), channels_in))\n",
    "    masks.append(np.tile(np.array(build_mask(1, 0, img_width - 1, img_width ** 2)), channels_in))\n",
    "    masks.append(np.tile(np.array(build_mask(0, 0, img_width ** 2, img_width ** 2)), channels_in))\n",
    "    masks.append(np.tile(np.array(build_mask(0, 1, img_width - 1, img_width ** 2)), channels_in))\n",
    "    masks.append(np.tile(np.array(build_mask(1, img_width - 1, img_width - 1, img_width ** 2)), channels_in))\n",
    "    masks.append(np.tile(np.array(build_mask(0, img_width, img_width ** 2, img_width ** 2)), channels_in))\n",
    "    masks.append(np.tile(np.array(build_mask(0, img_width + 1, img_width - 1, img_width ** 2)), channels_in))\n",
    "    return masks\n",
    "'''    \n",
    "# converts binary mask from stride 1 to stride s\n",
    "def altalena(mask, img_width, stride=2):\n",
    "    new_v = []\n",
    "    for i in range(len(mask)):\n",
    "        if i % stride != 0:\n",
    "            new_v.append(0)\n",
    "        elif i % (img_width*stride) >= img_width:\n",
    "            new_v.append(0)\n",
    "        else:\n",
    "            new_v.append(mask[i])\n",
    "    return new_v\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# this block works fine for the masks discussed in the paper, need to see under what conditions this holds generally\n",
    "# odd dimension kernel ? what size padding and stride ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initial Layer\n",
    "def initialLayer(img_width, kernel_size, channels_in, channels_out, conv_weight, bn, bin_masks):\n",
    "    #calculate weight values for affine bn\n",
    "    A = bn.weight / torch.sqrt(bn.running_var + bn.eps)\n",
    "    b = -(bn.weight * bn.running_mean / torch.sqrt(bn.running_var + bn.eps)) + bn.bias\n",
    "    print(b)\n",
    "    A = A.detach()\n",
    "    for i in range(channels_out):\n",
    "        kernels = [np.array([]) for z in range(kernel_size**2)]\n",
    "        for j in range(channels_in):\n",
    "            # specific to intial layer\n",
    "            weights = conv_weight[i][j].reshape(kernel_size**2)\n",
    "            for k in range(kernel_size**2):\n",
    "                kernels[k] = np.append(kernels[k], np.repeat(weights[k].detach(), img_width**2))\n",
    "        # fill remaining slots with 0s\n",
    "        for j in range(channels_out-channels_in):\n",
    "            for k in range(kernel_size**2):\n",
    "                kernels[k] = np.append(kernels[k], np.repeat(0, img_width**2))\n",
    "        # mask and save\n",
    "        for k in range(kernel_size**2):\n",
    "            kernels[k] = np.multiply(kernels[k], bin_masks[k])\n",
    "            kernels[k] = np.multiply(kernels[k], np.repeat(A[i], channels_out*(img_width**2)))\n",
    "            np.savetxt('../weights/conv1bn1-ch{}-k{}.bin'.format(i, k+1), kernels[k], delimiter=',')\n",
    "    np.savetxt('../weights/conv1bn1-bias.bin', np.repeat(b.detach(), img_width**2), delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convbn weight encoding\n",
    "def convbn(img_width, kernel_size, channels_in, channels_out, conv_weight, bn, layerNum, convNum, bin_masks):\n",
    "    A = bn.weight / torch.sqrt(bn.running_var + bn.eps)\n",
    "    b = -(bn.weight * bn.running_mean / torch.sqrt(bn.running_var + bn.eps)) + bn.bias\n",
    "    A = A.detach()\n",
    "    for i in range(channels_in):\n",
    "        ## build repeated kernel weights\n",
    "        kernels = [np.array([]) for z in range(kernel_size**2)]\n",
    "        for j in range(channels_out):\n",
    "            weights = conv_weight[j][(j+i)%channels_in].reshape(kernel_size**2)\n",
    "            for k in range(kernel_size**2):\n",
    "                kernels[k] = np.append(kernels[k], np.repeat(weights[k].detach(), img_width**2))\n",
    "        \n",
    "        ## apply binary masks to allow for padding\n",
    "        for k in range(kernel_size**2):\n",
    "            kernels[k] = np.multiply(kernels[k], bin_masks[k])\n",
    "            kernels[k] = np.multiply(kernels[k], np.repeat(A, img_width**2))\n",
    "            kernels[k] = np.roll(kernels[k], (img_width**2)*i)\n",
    "            np.savetxt('../weights/layer{}-conv{}bn{}-ch{}-k{}.bin'.format(layerNum, convNum, convNum, i,k+1), kernels[k], delimiter=',')\n",
    "        \n",
    "    # save biases\n",
    "    np.savetxt('../weights/layer{}-conv{}bn{}-bias.bin'.format(layerNum, convNum, convNum), np.repeat(b.detach(), img_width**2), delimiter=',')\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def downsamplingConvbn(img_width, kernel_size, channels_in, channels_out, conv_weight, bn, layerNum, convNum, bin_masks):\n",
    "    A = bn.weight / torch.sqrt(bn.running_var + bn.eps)\n",
    "    b = -(bn.weight * bn.running_mean / torch.sqrt(bn.running_var + bn.eps)) + bn.bias\n",
    "    A = A.detach()\n",
    "    for i in range(channels_in):\n",
    "        ## build repeated kernel weights\n",
    "        kernels = [np.array([]) for z in range(kernel_size**2)]\n",
    "        for j in range(channels_out):\n",
    "            weights = conv_weight[j][(j+i)%channels_in].reshape(kernel_size**2)\n",
    "            for k in range(kernel_size**2):\n",
    "                kernels[k] = np.append(kernels[k], np.repeat(weights[k].detach(), img_width**2))\n",
    "        \n",
    "        ## apply binary masks to allow for padding\n",
    "        for k in range(kernel_size**2):\n",
    "            kernels[k] = np.multiply(kernels[k], altalena(np.tile(bin_masks[k], 2), img_width))\n",
    "            kernels[k] = np.multiply(kernels[k], np.repeat(A.numpy(), img_width**2))\n",
    "            kernels[k] = np.add(kernels[k], np.roll(kernels[k], (img_width**2)*(channels_in*-1)+1))[:(img_width**2)*(channels_in)]\n",
    "            np.savetxt('../weights/layer{}-conv{}bn{}-ch{}-k{}.bin'.format(layerNum, convNum, convNum, i,k+1), altalena(np.roll(kernels[k], (img_width**2)*i), img_width), delimiter=',')\n",
    "            np.savetxt('../weights/layer{}-conv{}bn{}-ch{}-k{}.bin'.format(layerNum, convNum, convNum, i+channels_in,k+1), altalena(np.roll(kernels[k], (i*(img_width**2))-1), img_width), delimiter=',')\n",
    "    bs = b.detach().numpy()\n",
    "    bias_corrected1 = altalena(np.repeat(bs[:int(channels_out/2)], img_width**2),img_width)\n",
    "    bias_corrected2 = altalena(np.roll(np.repeat(bs[int(channels_out/2):channels_out], img_width**2), -1), img_width)\n",
    "    np.savetxt('../weights/layer{}-conv{}bn{}-bias1.bin'.format(layerNum, convNum, convNum), bias_corrected1, delimiter=',')\n",
    "    np.savetxt('../weights/layer{}-conv{}bn{}-bias2.bin'.format(layerNum, convNum, convNum), bias_corrected2, delimiter=',')\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dx(img_width, channels_in, channels_out, downsample_weight, downsample_bias, layerNum, convNum, bin_masks):\n",
    "    \n",
    "    A = downsample_bias.weight / torch.sqrt(downsample_bias.running_var + downsample_bias.eps)\n",
    "    b = -(downsample_bias.weight * downsample_bias.running_mean / torch.sqrt(downsample_bias.running_var + downsample_bias.eps)) + downsample_bias.bias\n",
    "    A = A.detach()\n",
    "    for i in range(channels_in):\n",
    "        kernel = np.array([])\n",
    "        for j in range(channels_out):\n",
    "            weight = downsample_weight[j][(j+i)%channels_in].reshape(1)[0]\n",
    "            kernel = np.append(kernel, np.repeat(weight.detach(), img_width**2))\n",
    "        \n",
    "        kernel = np.multiply(kernel, altalena(np.tile(bin_masks[4],2), img_width))\n",
    "        kernel = np.multiply(kernel, np.repeat(A.numpy(), img_width**2))\n",
    "        kernel = np.add(kernel, np.roll(kernel, (img_width**2)*(channels_in*-1)+1))[:(img_width**2)*(channels_in)]\n",
    "        \n",
    "        np.savetxt('../weights/layer{}dx-conv{}bn{}-ch{}-k1.bin'.format(layerNum, convNum, convNum, i), altalena(np.roll(kernel, (img_width**2)*i), img_width), delimiter=',')\n",
    "        np.savetxt('../weights/layer{}dx-conv{}bn{}-ch{}-k1.bin'.format(layerNum, convNum, convNum, i+channels_in), altalena(np.roll(kernel, (i*(img_width**2))-1), img_width), delimiter=',')\n",
    "    bs = b.detach().numpy()\n",
    "    bias_corrected1 = altalena(np.repeat(bs[:int(channels_out/2)], img_width**2), img_width)\n",
    "    bias_corrected2 = altalena(np.repeat(bs[int(channels_out/2):channels_out], img_width**2), img_width)\n",
    "    np.savetxt('../weights/layer{}dx-conv{}bn{}-bias1.bin'.format(layerNum, convNum, convNum), bias_corrected1, delimiter=',')\n",
    "    np.savetxt('../weights/layer{}dx-conv{}bn{}-bias2.bin'.format(layerNum, convNum, convNum), bias_corrected2, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fc(fc_weight, i, features_out):\n",
    "    for j in range(features_out):\n",
    "        np.savetxt('../weights/fc{}-f{}.bin'.format(i, j+1), fc_weight[j].reshape(-1).detach().numpy())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for AlexNet:\n\tUnexpected key(s) in state_dict: \"fc3.weight\". \n\tsize mismatch for fc1.weight: copying a param with shape torch.Size([1000, 2048]) from checkpoint, the shape in current model is torch.Size([400, 2048]).\n\tsize mismatch for fc2.weight: copying a param with shape torch.Size([100, 1000]) from checkpoint, the shape in current model is torch.Size([10, 400]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 44\u001b[0m\n\u001b[1;32m     42\u001b[0m model \u001b[38;5;241m=\u001b[39m AlexNet()\n\u001b[1;32m     43\u001b[0m network_state_dict \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAlexNet.pth\u001b[39m\u001b[38;5;124m'\u001b[39m, map_location\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 44\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnetwork_state_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2189\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2184\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2185\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2186\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[1;32m   2188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2189\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2190\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   2191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for AlexNet:\n\tUnexpected key(s) in state_dict: \"fc3.weight\". \n\tsize mismatch for fc1.weight: copying a param with shape torch.Size([1000, 2048]) from checkpoint, the shape in current model is torch.Size([400, 2048]).\n\tsize mismatch for fc2.weight: copying a param with shape torch.Size([100, 1000]) from checkpoint, the shape in current model is torch.Size([10, 400])."
     ]
    }
   ],
   "source": [
    "class AlexNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AlexNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(64)\n",
    "        self.conv4 = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn4 = nn.BatchNorm2d(64) \n",
    "        self.conv5 = nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1, bias=False)\n",
    "        self.bn5 = nn.BatchNorm2d(128)\n",
    "  \n",
    "\n",
    "        self.fc1 = nn.Linear(128*4*4,400, bias =  False)\n",
    "        self.fc2 = nn.Linear(400, 10, bias = False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu (x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.bn4(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv5(x)\n",
    "        x = self.bn5(x)\n",
    "        x = F.relu(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "model = AlexNet()\n",
    "network_state_dict = torch.load('AlexNetSmallfc.pth', map_location='cpu')\n",
    "model.load_state_dict(network_state_dict)\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.8691,  0.2567,  0.7915,  0.4818,  1.0705, -0.6528,  0.7790,  0.0726,\n",
      "         0.1938, -0.6687,  0.2955,  0.0956,  0.2218,  0.3107,  0.4701,  0.0645],\n",
      "       grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_932737/3924698602.py:22: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
      "  kernels[k] = np.multiply(kernels[k], np.repeat(A[i], channels_out*(img_width**2)))\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (4096,) (3072,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 26\u001b[0m\n\u001b[1;32m     22\u001b[0m img_width \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(img_width\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     24\u001b[0m bin_masks \u001b[38;5;241m=\u001b[39m build_masks(img_width, kernel_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, out_channels \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m48\u001b[39m)\n\u001b[0;32m---> 26\u001b[0m \u001b[43mconvbn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_width\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchannels_in\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchannels_out\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m       \u001b[49m\u001b[43mconv_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv4\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbn4\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m       \u001b[49m\u001b[43mlayerNum\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvNum\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbin_masks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbin_masks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m downsamplingConvbn(img_width, kernel_size, channels_in\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m, channels_out\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m,\n\u001b[1;32m     32\u001b[0m                    conv_weight\u001b[38;5;241m=\u001b[39mmodel\u001b[38;5;241m.\u001b[39mconv5\u001b[38;5;241m.\u001b[39mweight, bn\u001b[38;5;241m=\u001b[39mmodel\u001b[38;5;241m.\u001b[39mbn5,\n\u001b[1;32m     33\u001b[0m                    layerNum\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, convNum\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, bin_masks\u001b[38;5;241m=\u001b[39mbin_masks)\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# layer\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[4], line 16\u001b[0m, in \u001b[0;36mconvbn\u001b[0;34m(img_width, kernel_size, channels_in, channels_out, conv_weight, bn, layerNum, convNum, bin_masks)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m## apply binary masks to allow for padding\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(kernel_size\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m):\n\u001b[0;32m---> 16\u001b[0m     kernels[k] \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmultiply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkernels\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbin_masks\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m     kernels[k] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmultiply(kernels[k], np\u001b[38;5;241m.\u001b[39mrepeat(A, img_width\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m))\n\u001b[1;32m     18\u001b[0m     kernels[k] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mroll(kernels[k], (img_width\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m*\u001b[39mi)\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (4096,) (3072,) "
     ]
    }
   ],
   "source": [
    "img_width = 32\n",
    "channels = 3\n",
    "kernel_size = 3\n",
    "bin_masks = build_masks(img_width, kernel_size=3, out_channels = 16)\n",
    "\n",
    "initialLayer(img_width, kernel_size, channels_in=channels, channels_out=16, \n",
    "             conv_weight=model.conv1.weight, bn=model.bn1, bin_masks=bin_masks)\n",
    "\n",
    "\n",
    "downsamplingConvbn(img_width, kernel_size, channels_in=16, channels_out=32,\n",
    "                   conv_weight=model.conv2.weight, bn=model.bn2,\n",
    "                   layerNum=2, convNum=1, bin_masks=bin_masks)\n",
    "\n",
    "img_width = int(img_width/2)\n",
    "\n",
    "bin_masks = build_masks(img_width, kernel_size=3, out_channels = 32)\n",
    "\n",
    "downsamplingConvbn(img_width, kernel_size, channels_in=32, channels_out=64,\n",
    "                   conv_weight=model.conv3.weight, bn=model.bn3,\n",
    "                   layerNum=3, convNum=1, bin_masks=bin_masks)\n",
    "\n",
    "img_width = int(img_width/2)\n",
    "\n",
    "bin_masks = build_masks(img_width, kernel_size=3, out_channels = 64)\n",
    "\n",
    "convbn(img_width, kernel_size, channels_in=64, channels_out=64, \n",
    "       conv_weight=model.conv4.weight, bn=model.bn4, \n",
    "       layerNum=4, convNum=1, bin_masks=bin_masks)\n",
    "\n",
    "\n",
    "downsamplingConvbn(img_width, kernel_size, channels_in=64, channels_out=128,\n",
    "                   conv_weight=model.conv5.weight, bn=model.bn5,\n",
    "                   layerNum=5, convNum=1, bin_masks=bin_masks)\n",
    "\n",
    "\n",
    "\n",
    "# layer\n",
    "fc(model.fc1.weight, 1, 400)\n",
    "fc(model.fc2.weight, 2, 10)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
